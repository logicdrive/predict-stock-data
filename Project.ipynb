{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxvaNDCAptOp"
      },
      "source": [
        "### 1. 동작 환경 세팅 및 학습 데이터 불러오기\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-eupL28I4o6"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pytorch-lightning ta\n",
        "!mkdir ./check_points ./lightning_logs ./lightning_logs/LSTM_MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K38MloNBCc7o"
      },
      "outputs": [],
      "source": [
        "!unzip \"./Huge_Stock_Market_Dataset.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLMnuhFIpygQ"
      },
      "source": [
        "### 2. 데이터 전처리 및 데이터 로더 생성시키기\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import ta\n",
        "\n",
        "# 전처리되지 않은 주가 데이터들을 처리하는 과정을 관리하기 위해서\n",
        "class Raw_Stock_Data_Processor :\n",
        "  def __init__(self) :\n",
        "    self.DATA_SETS = {\"X_DATAS\":np.array([]), \"Y_DATAS\":np.array([])}\n",
        "  \n",
        "\n",
        "  # 데이터를 전처리하고, 그 결과를 클래스 데이터로 저장시키기 위해서\n",
        "  def __processing_Raw_Stock_Data(self, csv_path, used_prev_days=30, pred_after_days=7) :\n",
        "    CSV_DATA = pd.read_csv(csv_path).loc[:, [\"Open\", \"High\", \"Low\", \"Close\"]] + 1e-6\n",
        "\n",
        "    CSV_DATA = Raw_Stock_Data_Processor.add_Technic_Indexes(CSV_DATA)\n",
        "    CSV_DATA = CSV_DATA.iloc[30:]\n",
        "\n",
        "    LABELS = ((CSV_DATA[\"Close\"].shift(-pred_after_days+1) - CSV_DATA[\"Open\"]) / CSV_DATA[\"Open\"]).dropna().tolist()\n",
        "    DATAS = CSV_DATA.drop([\"Open\", \"High\", \"Low\", \"Close\"], axis=1).values.tolist()\n",
        "\n",
        "\n",
        "    x_datas = []\n",
        "    y_datas = []\n",
        "    for data_index in range(len(DATAS)-used_prev_days-pred_after_days) :\n",
        "      DATAS_CHANNEL = DATAS[data_index:data_index+used_prev_days]\n",
        "      x_datas.append(np.stack([DATAS_CHANNEL, DATAS_CHANNEL, DATAS_CHANNEL]))\n",
        "      y_datas.append(LABELS[data_index+used_prev_days+1])\n",
        "\n",
        "    return np.array(x_datas, dtype=np.float32), np.array(y_datas, dtype=np.float32)\n",
        "  \n",
        "  # 다수의 경로에 존재하는 데이터들을 전처리하고 그 결과를 합치기 위해서\n",
        "  def processing_Raw_Stock_Datas(self, csv_paths, used_prev_days=30, pred_after_days=7) :\n",
        "    x_datas_stack, y_datas_stack = [], []\n",
        "    for csv_path_index, csv_path in enumerate(csv_paths) :\n",
        "      print(f\"[*] {csv_path_index+1}/{len(csv_paths)} 번째 csv 파일 처리중... : {csv_path}\")\n",
        "      X_DATAS, Y_DATAS = self.__processing_Raw_Stock_Data(csv_path, used_prev_days, pred_after_days)\n",
        "      if len(X_DATAS.shape) != 4 : continue\n",
        "\n",
        "      x_datas_stack.append(X_DATAS)\n",
        "      y_datas_stack.append(Y_DATAS)\n",
        "\n",
        "    self.DATA_SETS[\"X_DATAS\"] = np.concatenate(x_datas_stack, axis=0)\n",
        "    self.DATA_SETS[\"Y_DATAS\"] = np.concatenate(y_datas_stack, axis=0)\n",
        "\n",
        "    print(self.DATA_SETS[\"X_DATAS\"].shape)\n",
        "    print(self.DATA_SETS[\"Y_DATAS\"].shape)\n",
        "  \n",
        "  \n",
        "  # 사용 할 주요 지표들을 추가시키기 위해서\n",
        "  @staticmethod\n",
        "  def add_Technic_Indexes(csv_datas) :\n",
        "    csv_datas[\"Open_Pct\"] = csv_datas[\"Open\"].pct_change()\n",
        "    csv_datas[\"High_Pct\"] = csv_datas[\"High\"].pct_change()\n",
        "    csv_datas[\"Low_Pct\"] = csv_datas[\"Low\"].pct_change()\n",
        "    csv_datas[\"Close_Pct\"] = csv_datas[\"Close\"].pct_change()\n",
        "\n",
        "    csv_datas[\"Open_Close_Pct\"] = (csv_datas[\"Close\"] - csv_datas[\"Open\"]) / csv_datas[\"Open\"]\n",
        "    csv_datas[\"High_Low_Pct\"] = (csv_datas[\"High\"] - csv_datas[\"Low\"]) / csv_datas[\"Low\"]\n",
        "\n",
        "    csv_datas[\"Close_WMA_5_Pct\"] = ta.trend.WMAIndicator(close=csv_datas[\"Close\"], window=5).wma().pct_change()\n",
        "    csv_datas[\"Close_WMA_10_Pct\"] = ta.trend.WMAIndicator(close=csv_datas[\"Close\"], window=10).wma().pct_change()\n",
        "\n",
        "    csv_datas[\"BB_High_Band_Pct\"] = ta.volatility.BollingerBands(close=csv_datas[\"Close\"], window=20).bollinger_hband().pct_change()\n",
        "    csv_datas[\"BB_Low_Band_Pct\"] = ta.volatility.BollingerBands(close=csv_datas[\"Close\"], window=20).bollinger_lband().pct_change()\n",
        "    csv_datas[\"BB_Channel_Pct\"] = ta.volatility.BollingerBands(close=csv_datas[\"Close\"], window=20).bollinger_pband()\n",
        "    csv_datas[\"BB_Width_Pct\"] = ta.volatility.BollingerBands(close=csv_datas[\"Close\"], window=20).bollinger_wband().pct_change()\n",
        "\n",
        "    csv_datas[\"MACD_Pct\"] = ta.trend.MACD(close=csv_datas[\"Close\"]).macd().pct_change()\n",
        "    csv_datas[\"RSI_Pct\"] = ta.momentum.RSIIndicator(close=csv_datas[\"Close\"], window=20).rsi().pct_change()\n",
        "    csv_datas[\"CCI_Pct\"] = ta.trend.CCIIndicator(high=csv_datas[\"High\"], low=csv_datas[\"Low\"], close=csv_datas[\"Close\"], window=20).cci().pct_change()\n",
        "    csv_datas[\"ATR_Pct\"] = ta.volatility.AverageTrueRange(high=csv_datas[\"High\"], low=csv_datas[\"Low\"], close=csv_datas[\"Close\"], window=20).average_true_range().pct_change()\n",
        "    return csv_datas\n",
        "\n",
        "\n",
        "  # 전처리한 데이터를 파일로 저장시키기 위해서\n",
        "  def save(self, save_path) :\n",
        "    if len(self.DATA_SETS[\"X_DATAS\"]) == 0 or len(self.DATA_SETS[\"Y_DATAS\"]) == 0 : raise Exception(\"Datas to save are not exist !\")  \n",
        "    np.save(save_path, self.DATA_SETS)\n",
        "  \n",
        "  # 이미 전처리된 데이터를 불러오기 위해서\n",
        "  def load(self, load_path) :\n",
        "    self.DATA_SETS = np.load(load_path, allow_pickle=True).item()\n",
        "  \n",
        "  \n",
        "  def __len__(self) :\n",
        "    return len(self.DATA_SETS[\"X_DATAS\"])\n",
        "\n",
        "  def __getitem__(self, name):\n",
        "    return self.DATA_SETS[name]"
      ],
      "metadata": {
        "id": "5VaFIWvde72u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "k7JSKhbLncgH"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import Dataset\n",
        "import math\n",
        "\n",
        "# 전처리된 주가 데이터들의 일부분을 데이터셋으로 생성시키기 위해서\n",
        "class Stock_Dataset(Dataset) :\n",
        "  def __init__(self, raw_stock_data_processor, div_ratio_start=0.0, div_ratio_end=1.0, unit=32) :\n",
        "    START_INDEX, END_INDEX = int(len(raw_stock_data_processor)*div_ratio_start), int(len(raw_stock_data_processor)*div_ratio_end)\n",
        "    ADJUST_END_INDEX = START_INDEX + math.floor((END_INDEX-START_INDEX)/unit)*unit\n",
        "    self.X_DATAS = raw_stock_data_processor[\"X_DATAS\"][START_INDEX:ADJUST_END_INDEX]\n",
        "    self.Y_DATAS = raw_stock_data_processor[\"Y_DATAS\"][START_INDEX:ADJUST_END_INDEX]\n",
        "\n",
        "  def __len__(self) :\n",
        "    return len(self.X_DATAS)\n",
        "  \n",
        "  def __getitem__(self, i) :\n",
        "    return self.X_DATAS[i], self.Y_DATAS[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo8Z68CPnSrO"
      },
      "outputs": [],
      "source": [
        "# 실제 주가 데이터들을 학습에 적합하도록 전처리시키기 위해서\n",
        "RAW_STOCK_DATA_PROCESSOR = Raw_Stock_Data_Processor()\n",
        "RAW_STOCK_DATA_PROCESSOR.processing_Raw_Stock_Datas([\"/content/Huge_Stock_Market_Dataset/Stocks/a.us.txt\"], 30, 7)\n",
        "RAW_STOCK_DATA_PROCESSOR.save(\"train_sets_a_us.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-3p4JUQYrWG"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "# 전처리된 주가 데이터들을 각각 학습 / 검증 / 테스트 데이터셋으로 생성시키기 위해서\n",
        "RAW_STOCK_DATA_PROCESSOR = Raw_Stock_Data_Processor()\n",
        "RAW_STOCK_DATA_PROCESSOR.load(\"/content/train_sets_a_us.npy\")\n",
        "\n",
        "\n",
        "# 학습, 검증, 테스트 데이터셋 불러오기 및 데이터 로더 생성하기\n",
        "TRAIN_DATASET = Stock_Dataset(RAW_STOCK_DATA_PROCESSOR, 0.0, 0.6, 32)\n",
        "VALID_DATASET = Stock_Dataset(RAW_STOCK_DATA_PROCESSOR, 0.6, 0.8, 32)\n",
        "TEST_DATASET = Stock_Dataset(RAW_STOCK_DATA_PROCESSOR, 0.8, 1.0, 32)\n",
        "\n",
        "TRAIN_LOADER = DataLoader(TRAIN_DATASET, batch_size=32, shuffle=True, drop_last=True)\n",
        "VALID_LOADER = DataLoader(VALID_DATASET, batch_size=32, shuffle=False, drop_last=True)\n",
        "TEST_LOADER = DataLoader(TEST_DATASET, batch_size=32, shuffle=False, drop_last=True)\n",
        "\n",
        "len(TRAIN_DATASET), len(VALID_DATASET), len(TEST_DATASET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn21PrckJGDH"
      },
      "source": [
        "### 3. 신경망 생성하기\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.adam import Adam\n",
        "\n",
        "# 특성들을 조합시키고, 크기를 조금씩 감소시키기 위한 레이어\n",
        "class Basic_ResNet_Downsample_Layer(nn.Module) :\n",
        "  def __init__(self, in_channels, out_channels, kernel_width, padding_width) :\n",
        "    super().__init__()\n",
        "    \n",
        "    self.CONV_1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3, kernel_width), padding=(1, padding_width))\n",
        "    self.CONV_2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(3, 3), padding=1)\n",
        "\n",
        "    self.BN_1 = nn.BatchNorm2d(num_features=out_channels)\n",
        "    self.BN_2 = nn.BatchNorm2d(num_features=out_channels)\n",
        "\n",
        "    self.LERU = nn.ReLU()\n",
        "\n",
        "    self.CONV_DOWN_SAMPLE = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3, kernel_width), padding=(1, padding_width))\n",
        "\n",
        "    self.LAYER_SEQ = nn.Sequential(\n",
        "      self.CONV_1, self.BN_1, self.LERU,\n",
        "      self.CONV_2, self.BN_2\n",
        "    )\n",
        "  \n",
        "  def forward(self, x) :\n",
        "    x = self.LAYER_SEQ(x) + self.CONV_DOWN_SAMPLE(x)\n",
        "    x = self.LERU(x)\n",
        "    return x\n",
        "\n",
        "# 순서가 있는 특성을 추출하기 위한 레이어\n",
        "class Basic_Self_Attension_Skip_Layer(nn.Module) :\n",
        "  def __init__(self, input_size) :\n",
        "    super().__init__()\n",
        "\n",
        "    self.ATTENTION = torch.nn.MultiheadAttention(input_size, 8)\n",
        "    self.BN = nn.BatchNorm1d(num_features=32)\n",
        "  \n",
        "  def forward(self, x) :\n",
        "    x_ = x\n",
        "    x, _ = self.ATTENTION(x, x, x)\n",
        "    x = self.BN(x)\n",
        "    return x + x_\n",
        "\n",
        "class LSTM_Module(pl.LightningModule) :\n",
        "  def __init__(self) :\n",
        "    super(LSTM_Module, self).__init__()\n",
        "\n",
        "    res_net_layers = []\n",
        "    res_net_layers.append(Basic_ResNet_Downsample_Layer(in_channels=3, out_channels=32, kernel_width=16, padding_width=7))\n",
        "    for layer_index in range(6+1) :\n",
        "      res_net_layers.append(Basic_ResNet_Downsample_Layer(in_channels=32, out_channels=32, kernel_width=15-layer_index*2, padding_width=6-layer_index))\n",
        "    self.RES_NET_DOWNSAMPLE_SEQ = nn.Sequential(*res_net_layers)\n",
        "\n",
        "    attenion_layers = [Basic_Self_Attension_Skip_Layer(32) for _ in range(3)]\n",
        "    self.SELF_ATTENTION_SKIP_SEQ = nn.Sequential(*attenion_layers)\n",
        "\n",
        "    self.FC_1 = nn.Linear(960, 480)\n",
        "    self.FC_2 = nn.Linear(480, 1)\n",
        "    self.LERU = nn.ReLU()\n",
        "    self.FC_LAYER_SEQ = nn.Sequential(self.FC_1, self.LERU, self.FC_2)\n",
        "  \n",
        "  def forward(self, x) :\n",
        "    x = self.RES_NET_DOWNSAMPLE_SEQ(x)\n",
        "\n",
        "    x = x.view(x.shape[0], 32, -1) \n",
        "    x = x.permute(2, 0, 1)\n",
        "\n",
        "    x = self.SELF_ATTENTION_SKIP_SEQ(x)\n",
        "    x = x.permute(1, 0, 2)\n",
        "    x = torch.reshape(x, (x.shape[0], -1))\n",
        "\n",
        "    x = self.FC_LAYER_SEQ(x)\n",
        "    return x\n",
        "  \n",
        "\n",
        "  def training_step(self, batch, batch_idx) :\n",
        "    LOSS = self.__calc_Loss(batch)\n",
        "    self.log('train_loss', LOSS, on_epoch=True, prog_bar=True)\n",
        "    return LOSS\n",
        "  \n",
        "  def validation_step(self, batch, batch_idx) :\n",
        "    self.log('valid_loss', self.__calc_Loss(batch), on_epoch=True, prog_bar=True)\n",
        "  \n",
        "  def test_step(self, batch, batch_idx) :\n",
        "    self.log('test_loss', self.__calc_Loss(batch), on_epoch=True, prog_bar=True)\n",
        "  \n",
        "  def predict_step(self, batch, batch_idx) :\n",
        "    X, Y = batch\n",
        "    return self(X)\n",
        "\n",
        "\n",
        "  def __calc_Loss(self, batch) :\n",
        "    X, Y = batch\n",
        "    PREDS = self(X).flatten()\n",
        "    return nn.MSELoss()(PREDS, Y)\n",
        "  \n",
        "\n",
        "  def configure_optimizers(self):\n",
        "      return Adam(self.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "Zvu22QRIM7GI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MAERP-QLDiX"
      },
      "source": [
        "### 4. 신경망 학습하기\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etuMmX2LKT5d"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H49AHYEPKX6J"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "CHECK_POINT_CALLBACK = ModelCheckpoint(dirpath=\"./check_points\", monitor=\"valid_loss\", mode=\"min\", filename=\"lstm-{epoch:02d}-{valid_loss:.10f}\")\n",
        "LOGGER = pl.loggers.TensorBoardLogger(name=f'LSTM_MODEL', save_dir='lightning_logs')\n",
        "\n",
        "TRAINER = pl.Trainer(max_epochs=300, accelerator=DEVICE, callbacks=[CHECK_POINT_CALLBACK], logger=LOGGER)\n",
        "\n",
        "\n",
        "LSTM_MODULE = LSTM_Module()\n",
        "TRAINER.fit(LSTM_MODULE, train_dataloaders=TRAIN_LOADER, val_dataloaders=VALID_LOADER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdm5NZj9ZVOf"
      },
      "source": [
        "### 5. 모델 테스트하기\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIq1jXTNZZEl"
      },
      "outputs": [],
      "source": [
        "MODULE_TO_TEST = LSTM_Module.load_from_checkpoint(\"/content/check_points/lstm-epoch=228-valid_loss=0.0031873265.ckpt\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "TRAINER = pl.Trainer(accelerator=DEVICE)\n",
        "\n",
        "TRAINER.test(MODULE_TO_TEST, dataloaders=TRAIN_LOADER)\n",
        "TRAINER.test(MODULE_TO_TEST, dataloaders=VALID_LOADER)\n",
        "TRAINER.test(MODULE_TO_TEST, dataloaders=TEST_LOADER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBKupAGCZ7Kk"
      },
      "source": [
        "### 7. 모델로 실예측 해보기\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_D4tFL3LgBuc"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "# 데이터로더루부터 예측한 값 및 정답을 1차원 형태로 추출하기 위해서\n",
        "def predict_From_Dataloader(trainer, model_to_pred, dataloader) :\n",
        "  PREDS = trainer.predict(model_to_pred, dataloaders=dataloader)\n",
        "  FLATTEN_PREDS = torch.cat([pred.flatten() for pred in PREDS]).tolist()\n",
        "  FLATTEN_LABELS = dataloader.dataset[:][1].flatten().tolist()\n",
        "  return FLATTEN_PREDS, FLATTEN_LABELS\n",
        "\n",
        "# 훈련 데이터와 예측된 값을 통해서 조정값을 얻기위해서\n",
        "def get_Adjust_Values(train_preds, train_labels) :\n",
        "  ADJUST_MEAN = statistics.mean(train_preds)\n",
        "  ADJUST_STD_DEV = statistics.stdev(train_labels)/statistics.stdev(train_preds)\n",
        "  return ADJUST_MEAN, ADJUST_STD_DEV\n",
        "\n",
        "# 학습데이터를 기반으로 얻어진 정규화 값을 이용해서 값을 맞춰주기 위해서\n",
        "def adjust_Preds(adjust_mean, adjust_std_dev, preds_to_adjust) :\n",
        "  return list(map(lambda e : (e - adjust_mean)*adjust_std_dev, preds_to_adjust))\n",
        "\n",
        "# 얻어진 예측, 정답 값을 그래프 형태로 출력시키기 위해서\n",
        "def show_Graphs(preds, preds_name, labels, labels_name) :\n",
        "  figures = []\n",
        "\n",
        "  figures.append(go.Scatter(\n",
        "    y=preds,\n",
        "    name=preds_name\n",
        "  ))\n",
        "  figures.append(go.Scatter(\n",
        "    y=labels,\n",
        "    name=labels_name\n",
        "  ))\n",
        "\n",
        "  go.Figure(figures).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6yL7pTUZtC3"
      },
      "outputs": [],
      "source": [
        "MODULE_TO_PRED = LSTM_Module.load_from_checkpoint(\"/content/check_points/lstm-epoch=228-valid_loss=0.0031873265.ckpt\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "TRAINER = pl.Trainer(accelerator=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PREDS, TRAIN_LABELS = predict_From_Dataloader(TRAINER, MODULE_TO_PRED, TRAIN_LOADER)\n",
        "ADJUST_MEAN, ADJUST_STD_DEV = get_Adjust_Values(TRAIN_PREDS, TRAIN_LABELS)\n",
        "ADJUST_MEAN, ADJUST_STD_DEV"
      ],
      "metadata": {
        "id": "5kbZDuxrSC5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95FQEGwkZ_WK"
      },
      "outputs": [],
      "source": [
        "VALID_PREDS, VALID_LABELS = predict_From_Dataloader(TRAINER, MODULE_TO_PRED, VALID_LOADER)\n",
        "TEST_PREDS, TEST_LABELS = predict_From_Dataloader(TRAINER, MODULE_TO_PRED, TEST_LOADER)\n",
        "\n",
        "ADJUST_TRAIN_PREDS = adjust_Preds(ADJUST_MEAN, ADJUST_STD_DEV, TRAIN_PREDS)\n",
        "ADJUST_VALID_PREDS = adjust_Preds(ADJUST_MEAN, ADJUST_STD_DEV, VALID_PREDS)\n",
        "ADJUST_TEST_PREDS = adjust_Preds(ADJUST_MEAN, ADJUST_STD_DEV, TEST_PREDS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ADJUST_MEAN, ADJUST_STD_DEV"
      ],
      "metadata": {
        "id": "fmEIVdezSJ1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDWmHf97eFz_"
      },
      "outputs": [],
      "source": [
        "MAX_SHOW_INDEX = 1000\n",
        "show_Graphs(ADJUST_TRAIN_PREDS[:MAX_SHOW_INDEX], \"adjust_train_preds\", TRAIN_LABELS[:MAX_SHOW_INDEX], \"train_labels\")\n",
        "show_Graphs(ADJUST_VALID_PREDS[:MAX_SHOW_INDEX], \"adjust_valid_preds\", VALID_LABELS[:MAX_SHOW_INDEX], \"valid_labels\")\n",
        "show_Graphs(ADJUST_TEST_PREDS[:MAX_SHOW_INDEX], \"adjust_test_preds\", TEST_LABELS[:MAX_SHOW_INDEX], \"test_labels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpIVWVY4pTG_"
      },
      "source": [
        "### 8. 예측을 통한 투자시뮬레이션 해보기\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2dF8aqiepJSm"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class Predict_Next_Percentage :\n",
        "  def __init__(self, model_path) :\n",
        "    self.MODEL = LSTM_Module.load_from_checkpoint(model_path)\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    self.TRAINER = pl.Trainer(accelerator=DEVICE, enable_progress_bar=False, logger=False)\n",
        "\n",
        "  # 주어진 데이터 프레임들을 전처리시킨 결과를 반환하기 위해서\n",
        "  def processing_Raw_Stock_Data(self, chart_train_datas, used_prev_days) :\n",
        "    datas = chart_train_datas.drop([\"Open\", \"High\", \"Low\", \"Close\"], axis=1).values.tolist()\n",
        "\n",
        "    x_datas = []\n",
        "    for data_index in range(len(datas)-used_prev_days+1) :\n",
        "      DATAS_CHANNEL = np.array(datas[data_index:data_index+used_prev_days], dtype=np.float32)\n",
        "      x_datas.append(torch.tensor(np.stack([DATAS_CHANNEL, DATAS_CHANNEL, DATAS_CHANNEL])))\n",
        "    return x_datas\n",
        "  \n",
        "  # 다음 퍼센테이지들을 순차적으로 예측시키기 위해서\n",
        "  def predict_Next_Percentages(self, chart_train_datas, used_prev_days, adjust_mean, adjust_std_dev, unit=32) :\n",
        "      PROCEED_STOCK_DATAS = self.processing_Raw_Stock_Data(chart_train_datas, used_prev_days)\n",
        "\n",
        "\n",
        "      DATA_PREDICT_LEN = len(PROCEED_STOCK_DATAS)\n",
        "      BATCH_DATA_PREDICT_LEN = math.ceil(DATA_PREDICT_LEN/unit)*unit\n",
        "\n",
        "      x_batch_datas = [proceed_stock_data for proceed_stock_data in PROCEED_STOCK_DATAS]\n",
        "      while len(x_batch_datas) != BATCH_DATA_PREDICT_LEN :\n",
        "        x_batch_datas.append(PROCEED_STOCK_DATAS[-1])\n",
        "        \n",
        "\n",
        "      DATAS_STACK = torch.stack(x_batch_datas)\n",
        "      DATASET = TensorDataset(DATAS_STACK, torch.zeros(BATCH_DATA_PREDICT_LEN))\n",
        "      DATA_LODAER = DataLoader(DATASET, batch_size=unit)\n",
        "      \n",
        "      PREDS = self.TRAINER.predict(self.MODEL, dataloaders=DATA_LODAER)\n",
        "      FLATTEN_PREDS = torch.stack(PREDS).flatten().tolist()[:DATA_PREDICT_LEN]\n",
        "      return [(pred - adjust_mean)*adjust_std_dev for pred in FLATTEN_PREDS]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USED_PREV_DAYS = 30 # 사용되는 이전 날짜\n",
        "DATA_UNIT = 32 # 배치가 되는 데이터의 단위\n",
        "PREV_AFTER_DAYS = 7 # 예측하는 변화율의 총 시간"
      ],
      "metadata": {
        "id": "hOjkSMTVBzbD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g_PWz0DLy9m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "CHART_DATAS = pd.read_csv(\"/content/Huge_Stock_Market_Dataset/Stocks/a.us.txt\")[:2656]\n",
        "# CHART_DATAS = pd.read_csv(\"/content/Huge_Stock_Market_Dataset/Stocks/a.us.txt\")[2656:]\n",
        "\n",
        "CHART_DATAS = CHART_DATAS.loc[:, [\"Open\", \"High\", \"Low\", \"Close\"]] + 1e-6\n",
        "CHART_DATAS = Raw_Stock_Data_Processor.add_Technic_Indexes(CHART_DATAS)\n",
        "CHART_DATAS = CHART_DATAS[30:]\n",
        "\n",
        "PREDICT_NEXT_PERCENTAGE = Predict_Next_Percentage(\"/content/check_points/lstm-epoch=228-valid_loss=0.0031873265.ckpt\")\n",
        "PERCENTAGE_PREDS = PREDICT_NEXT_PERCENTAGE.predict_Next_Percentages(CHART_DATAS, USED_PREV_DAYS, ADJUST_MEAN, ADJUST_STD_DEV, DATA_UNIT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SLICED_BEHAVE_PREDS = np.array(PERCENTAGE_PREDS[:-PREV_AFTER_DAYS:PREV_AFTER_DAYS]) > 0.00\n",
        "SLICED_ADVANTAGES = ((CHART_DATAS[\"Close\"].shift(-PREV_AFTER_DAYS+1) - CHART_DATAS[\"Open\"]) / CHART_DATAS[\"Open\"]).dropna().tolist()[USED_PREV_DAYS+1::PREV_AFTER_DAYS]\n",
        "SIMULATED_REVENU = np.cumsum(SLICED_ADVANTAGES * SLICED_BEHAVE_PREDS)"
      ],
      "metadata": {
        "id": "XhWjofINBxMR"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFe04Fb05p3k"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "CLOSE_GRAPH = CHART_DATAS[\"Close\"].tolist()[USED_PREV_DAYS::PREV_AFTER_DAYS]\n",
        "HAVE_REVEN_GRAPH = [0] + np.cumsum(SLICED_ADVANTAGES)\n",
        "SIMUL_REVEN_GRAPH = [0] + SIMULATED_REVENU.tolist()\n",
        "\n",
        "\n",
        "figures = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "figures.add_trace(go.Scatter(y=CLOSE_GRAPH, name=\"Close\"), secondary_y=False)\n",
        "figures.add_trace(go.Scatter(y=HAVE_REVEN_GRAPH, name=\"have_revenu\"), secondary_y=True)\n",
        "figures.add_trace(go.Scatter(y=SIMUL_REVEN_GRAPH, name=\"simul_revenu\"), secondary_y=True)\n",
        "\n",
        "go.Figure(figures).show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "XxvaNDCAptOp",
        "mE032qxSGYnr",
        "Kn21PrckJGDH",
        "8MAERP-QLDiX",
        "fdm5NZj9ZVOf",
        "dBKupAGCZ7Kk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}